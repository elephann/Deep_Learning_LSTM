{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bx    by    bz    bl    bm    bn  bmag      vx      vy     vz    vmag  \\\n",
      "0  0.52  3.99 -2.53  0.12 -4.74 -0.33  4.75 -212.49  126.52  94.29  264.67   \n",
      "1 -0.46  2.83 -0.37  1.47 -2.49 -0.08  2.89 -205.30  121.80  91.70  255.72   \n",
      "2  0.63  3.69 -2.23  0.10 -4.36 -0.12  4.36 -208.85  119.43  89.61  256.73   \n",
      "\n",
      "     np    tpar    tper  goal  \n",
      "0  5.51  141.67  157.74     2  \n",
      "1  6.51  139.22  154.13     2  \n",
      "2  6.15  144.39  151.92     2  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from pandas import read_excel\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#does it make any difference?\n",
    "%matplotlib inline\n",
    "\n",
    "#Loading the raw dataset (ignore the first 2 columns A & B)\n",
    "dataset = read_excel(\"challenge_dataset.xlsx\", usecols=\"C:Q\")\n",
    "\n",
    "values = dataset.values\n",
    "\n",
    "# Lets print the first 3 rows of the dataset\n",
    "print(dataset.head(3))\n",
    "\n",
    "#Lets create a quick plot of each series (as a separate subplot) and see what we have\n",
    "i = 0\n",
    "Nc = values.shape[1] #number of columns\n",
    "#pyplot.figure()\n",
    "for group in range(0,Nc):\n",
    "    i += 1\n",
    "#    pyplot.subplot(Nc, 1, i)\n",
    "#    pyplot.plot(values[:, group])\n",
    "#    pyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
    "#    #print(\"dataset.columns[group]\", group, dataset.columns[group])\n",
    "#pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values.shape = (18245, 15)\n",
      "scaled.shape = (18245, 14)\n"
     ]
    }
   ],
   "source": [
    "# ensuring all the data is float\n",
    "values = values.astype('float32')\n",
    "print(\"values.shape =\", values.shape)\n",
    "\n",
    "# normalizing features (columns 1 to 13) NOTE: column 14 = goal (three classes: 0, 1, 2)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values[:,0:Nc-1])\n",
    "#scaled = values[:,0:Nc-1]\n",
    "\n",
    "print(\"scaled.shape =\", scaled.shape)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(scaled)\n",
    "\n",
    "#print('df:', df.head())\n",
    "#print('dataset:', dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X before reshape: (14596, 14)\n",
      "test_X  before reshape: (3649, 14)\n",
      "train_X.shape = (164, 89, 14, 1)\n",
      "test_X.shape  = (41, 89, 14, 1) \n",
      "\n",
      "train_Y before reshape: (14596, 3)\n",
      "test_Y  before reshape: (3649, 3)\n",
      "train_Y.shape = (164, 89, 3)\n",
      "test_Y.shape  = (41, 89, 3)\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test sets\n",
    "# taking 80% of the data for training\n",
    "n_samples = 89 #each feature has 89 samples\n",
    "n_series = int( values.shape[0]/n_samples )\n",
    "n_train = int(0.80 * n_series) * n_samples\n",
    "\n",
    "# inputs\n",
    "train_X = np.array(scaled[:n_train, :])\n",
    "test_X  = np.array(scaled[n_train:, :])\n",
    "# outpts (making sure outputs are arrays of integers)\n",
    "last_column = values.shape[1] - 1 #last column (goal) in he raw dataset\n",
    "train_Y = np.array(values[:n_train, last_column], dtype=int)\n",
    "test_Y  = np.array(values[n_train:, last_column], dtype=int)\n",
    "\n",
    "# Lets convert Y from its current shape (m,1) into a \"one-hot representation\" (m,3),\n",
    "# to make it suitable for a softmax classifier.\n",
    "# Now each row in Y_oh will be a one-hot vector giving the class (0, 1, or 2)\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "train_Y = convert_to_one_hot(train_Y, C = 3)\n",
    "test_Y  = convert_to_one_hot(test_Y,  C = 3)\n",
    "\n",
    "# reshape input data to be 3D [samples, timesteps, features]\n",
    "print(\"train_X before reshape:\", train_X.shape)\n",
    "print(\"test_X  before reshape:\", test_X.shape)\n",
    "train_X = train_X.reshape((int(train_X.shape[0]/n_samples), n_samples, train_X.shape[1], 1))\n",
    "test_X  = test_X.reshape( (int(test_X.shape[0] /n_samples), n_samples,  test_X.shape[1], 1))\n",
    "print(\"train_X.shape =\", train_X.shape)\n",
    "print(\"test_X.shape  =\", test_X.shape, \"\\n\")\n",
    "\n",
    "print(\"train_Y before reshape:\", train_Y.shape)\n",
    "print(\"test_Y  before reshape:\", test_Y.shape)\n",
    "train_Y = train_Y.reshape((int(train_Y.shape[0]/n_samples), n_samples, 3))\n",
    "test_Y  = test_Y.reshape( (int(test_Y.shape[0] /n_samples), n_samples, 3))\n",
    "print(\"train_Y.shape =\", train_Y.shape)\n",
    "print(\"test_Y.shape  =\", test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected time_distributed_17_input to have 5 dimensions, but got array with shape (164, 89, 14, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ebf3d3722f73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m history = model.fit(train_X, train_Y, epochs=20,\n\u001b[1;32m---> 56\u001b[1;33m                     validation_data=(test_X, test_Y), verbose=2, shuffle=False)\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;31m# plot history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abbas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abbas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abbas\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected time_distributed_17_input to have 5 dimensions, but got array with shape (164, 89, 14, 1)"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers import Activation, TimeDistributed, Dropout\n",
    "from keras.layers import Conv2D, Conv1D, Bidirectional, Flatten\n",
    "from keras.layers import MaxPooling1D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "\n",
    "# Meysam:\n",
    "input_Shape   = (89, 14, 1)\n",
    "input_Shape   = (164, 89, 14, 1)\n",
    "train_X_Shape = (164, 89, 14, 1)\n",
    "\n",
    "# build CNN/LSTM and train it.\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3,3), padding='same', kernel_initializer='Orthogonal'), input_shape=input_Shape)) \n",
    "model.add(Activation('elu')) #'tanh'?\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3,3), padding='same', kernel_initializer='Orthogonal'))) \n",
    "model.add(Activation('elu'))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3,3), padding='same', kernel_initializer='Orthogonal'))) \n",
    "model.add(Activation('elu'))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Conv1D(16, 3, padding='same', kernel_initializer='Orthogonal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling1D(pool_size=8))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(LSTM( 64, return_sequences= True, kernel_initializer='Orthogonal'))) \n",
    "model.add(Activation('elu'))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False, kernel_initializer='Orthogonal'))) \n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(1, activation='softmax')) #sigmoid\n",
    "\n",
    "adam = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0001)\n",
    "\n",
    "### loss='categorical_crossentropy', 'categorical_accuracy', 'msle'\n",
    "### metrics=['accuracy'], ['msle', 'mae']\n",
    "model.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Meysam\n",
    "\n",
    "\n",
    "# fit network\n",
    "history = model.fit(train_X, train_Y, epochs=20, batch_size=32,\n",
    "                    validation_data=(test_X, test_Y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "#summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
